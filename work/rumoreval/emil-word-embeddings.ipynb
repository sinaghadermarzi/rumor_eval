{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emiljoswin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "filenames = [\n",
    "    'charliehebdo',\n",
    "    'ferguson',\n",
    "    'illary',\n",
    "    'prince-toronto',\n",
    "    'sydneysiege',\n",
    "    'ebola-essien',\n",
    "    'germanwings-crash',\n",
    "    'ottawashooting',\n",
    "    'putinmissing'\n",
    "]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Global Vectors....\n",
      "Completed loading Global Vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_dict = {}\n",
    "n_features = 200\n",
    "\n",
    "def prepare_embeddings():\n",
    "    print(\"Loading Global Vectors....\")\n",
    "    if n_features == 100:\n",
    "        embeddings_file = 'glove.twitter.27B/glove.twitter.27B.100d.txt'\n",
    "    else:\n",
    "        embeddings_file = 'glove.twitter.27B/glove.twitter.27B.200d.txt'\n",
    "\n",
    "    f = []\n",
    "    with open(embeddings_file) as file:\n",
    "        f = file.readlines()\n",
    "\n",
    "    for fi in f:\n",
    "        line = fi.split()\n",
    "        embedding_dict[line[0]] = line[1:] # TODO - convert into float here itself\n",
    "\n",
    "prepare_embeddings()\n",
    "print(\"Completed loading Global Vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1193514\n",
      "['0.11928', '0.22012', '-0.12575', '-0.071592', '0.036457', '-0.14278', '0.1239', '0.20858', '-0.13643', '-0.58471', '0.33246', '-0.42235', '-0.96011', '-0.2813', '0.036594', '-0.10324', '-0.23895', '0.37437', '0.23307', '0.18362', '-0.31105', '0.32997', '-0.16289', '0.0059497', '0.10002', '0.98538', '0.77692', '0.5707', '0.18615', '0.43621', '0.064573', '0.1086', '-0.086082', '-0.026253', '-0.31894', '-0.53476', '-0.13421', '0.43249', '-0.25937', '0.13175', '-0.27095', '0.22287', '0.15417', '0.067198', '0.067832', '-0.15939', '-0.55298', '-0.15007', '-0.11094', '-0.739', '-0.71714', '-0.18907', '-0.12591', '0.21601', '0.24323', '0.52563', '-0.58062', '0.23899', '0.15097', '0.44639', '0.15569', '0.12019', '0.010435', '-0.20619', '0.19929', '-0.2096', '0.54248', '0.095416', '-0.10734', '-0.32039', '-0.087812', '-0.28497', '0.43925', '0.26048', '0.3382', '0.36391', '0.024349', '0.070522', '0.20768', '0.08272', '-0.13455', '0.47675', '0.7644', '0.77207', '-0.018584', '0.12416', '-0.090752', '-0.34804', '-0.0077039', '0.38969', '-0.09014', '0.15761', '-0.3133', '-0.40453', '0.38091', '-0.20651', '-0.35078', '-0.19825', '-0.41375', '-0.33688', '-0.20639', '-0.19625', '-0.14113', '0.053793', '0.42927', '-0.33189', '-0.10779', '0.19436', '0.028269', '0.0022178', '0.051235', '0.20527', '-0.23556', '0.17277', '0.029161', '-0.17744', '0.0066617', '-0.1637', '-0.085944', '0.51653', '0.85496', '-0.34557', '-0.58577', '0.051327', '0.51148', '-0.61494', '0.60196', '-0.132', '0.33425', '0.39787', '-0.27722', '0.03974', '-0.35725', '0.45152', '-0.20339', '-0.71456', '0.31905', '0.10903', '-0.91247', '-0.19419', '0.29809', '-0.077097', '-0.18102', '-0.17997', '-0.37808', '-0.19841', '0.28974', '0.40714', '-0.064447', '0.24242', '0.28905', '0.22104', '-6.6393', '0.10854', '0.39845', '0.69317', '-0.42004', '-0.45383', '-0.23744', '0.68604', '-0.34553', '0.14608', '-0.58861', '-0.063803', '0.26923', '0.0567', '-0.29636', '-0.052958', '-0.13682', '0.20893', '0.62885', '-0.33775', '0.10676', '0.062964', '0.26429', '-0.53897', '0.13937', '-0.23935', '0.0068861', '0.29318', '0.678', '-0.060501', '0.09235', '-0.26994', '-0.15304', '-0.11354', '-0.1389', '0.049825', '-0.49375', '0.22977', '-0.4127', '0.38837', '0.030925', '0.12642', '-0.081794', '0.064737', '0.30267', '0.39631', '0.19979', '0.29074']\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(embedding_dict))\n",
    "print(embedding_dict['for'])\n",
    "print('is' in embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_file(name):\n",
    "    filename = 'parsed_files/' + name + '.json'\n",
    "    # print(filename)\n",
    "\n",
    "    try:\n",
    "        with open(filename) as f:\n",
    "            tweets = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(\"Exception\")\n",
    "        print(e.message)\n",
    "\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "repliesText = []\n",
    "reply_labels = []\n",
    "corpus = []\n",
    "\n",
    "def prepare_data(filenames):\n",
    "    for name in filenames:\n",
    "        t = load_data_from_file(name)\n",
    "\n",
    "        for i in range(len(t[name])):\n",
    "            thread = t[name][i]\n",
    "            reply_list = thread['replies']\n",
    "            for reply in reply_list:\n",
    "                text = reply[\"text\"]\n",
    "                label = reply[\"label\"]\n",
    "                repliesText.append(text)\n",
    "                reply_labels.append(label)\n",
    "\n",
    "    # Data Cleaning\n",
    "    ps = PorterStemmer()\n",
    "    for oneReply in repliesText:\n",
    "        cleanText = re.sub('http(\\S)+', ' http ', oneReply)\n",
    "        cleanText = re.sub('@', '', cleanText)\n",
    "        cleanText = re.sub('#', '', cleanText)\n",
    "        cleanText = re.sub('\\?', ' ? ', cleanText)\n",
    "        cleanText = re.sub('.', ' . ', cleanText)\n",
    "        cleanText = re.sub('[0-9]+', ' number ', cleanText)\n",
    "\n",
    "        cleanText = cleanText.lower()\n",
    "\n",
    "        splitCleanText = cleanText.split()\n",
    "\n",
    "        str = \"\"\n",
    "        for oneWord in splitCleanText:\n",
    "#             oneWord = ps.stem(oneWord) # TODO - stemming is a too aggressive here\n",
    "            str += (' '+ oneWord)\n",
    "            # str = ' '.join(oneWord)\n",
    "\n",
    "        corpus.append(str)\n",
    "\n",
    "prepare_data(filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5046, 200)\n",
      "(200,)\n",
      "5046\n"
     ]
    }
   ],
   "source": [
    "# X = [[] for i in range(len(corpus))]\n",
    "X = np.zeros((5046, n_features))\n",
    "print(X.shape)\n",
    "print(X[0].shape)\n",
    "y = reply_labels\n",
    "# n_features = 100\n",
    "\n",
    "a = np.array([float(s) for s in embedding_dict['for']]).reshape(n_features, 1)\n",
    "\n",
    "print(len(corpus))\n",
    "for i in range(len(corpus)):\n",
    "    sentence = corpus[i]\n",
    "    # for sentence in corpus:\n",
    "    words = sentence.split()\n",
    "    x_ = np.zeros((n_features, 1))\n",
    "    l = len(words)\n",
    "    for word in words:\n",
    "        if word in embedding_dict:\n",
    "            feat = np.array([float(s) for s in embedding_dict[word]]).reshape(n_features, 1)\n",
    "            x_ = np.add(x_, feat)\n",
    "#             print(feat.shape)\n",
    "        else:\n",
    "            print(\"word not found\", word) # NOTHING YES!!\n",
    "#     x_ = np.divide(x_, l).reshape(n_features,)\n",
    "    x_ = x_.reshape(n_features,)\n",
    "    X[i] = x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.reshape(5046, n_features)\n",
    "# print(X.shape)\n",
    "# print(y.shape)\n",
    "\n",
    "# a = np.zeros((3, 2))\n",
    "# b = np.array([1, 3])\n",
    "# print(a)\n",
    "# print(b)\n",
    "# a[1] = b\n",
    "# print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3532, 200)\n",
      "(200,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiljoswin/anaconda3/lib/python3.6/site-packages/imblearn/base.py:306: UserWarning: The target type should be binary.\n",
      "  warnings.warn('The target type should be binary.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=4, n_jobs=2,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from collections import Counter\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_train[0].shape)\n",
    "\n",
    "smote_enn = SMOTE(random_state=2)\n",
    "X_resampled, y_resampled = smote_enn.fit_sample(X_train, y_train)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "num_trees = 4\n",
    "max_depth = 4\n",
    "clf = RandomForestClassifier(n_jobs=2, random_state=0, n_estimators=num_trees, max_depth=max_depth)\n",
    "# clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_resampled, y_resampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('comment', 1033), ('deny', 130), ('query', 137), ('support', 214)]\n",
      "[('comment', 986), ('deny', 528)]\n",
      "[[673 360   0   0]\n",
      " [ 86  44   0   0]\n",
      " [ 90  47   0   0]\n",
      " [137  77   0   0]]\n",
      "precision =  [0.68255578 0.08333333        nan        nan]\n",
      "recall =  [0.65150048 0.33846154 0.         0.        ]\n",
      "f1 score =  [0.38104088 0.03967538        nan        nan]\n",
      "RF accuracy =  0.47357992073976224\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emiljoswin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: RuntimeWarning: invalid value encountered in true_divide\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(sorted(Counter(y_test).items()))\n",
    "print(sorted(Counter(y_predict).items()))\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(cm)\n",
    "#print(len(reply_labels)\n",
    "recall = np.diag(cm) / np.sum(cm, axis=1)\n",
    "precision = np.diag(cm)/ np.sum(cm, axis=0)\n",
    "\n",
    "f1_score = 2 * (precision*recall)/(precision + recall + 1)\n",
    "acc = accuracy_score(y_test, y_predict)\n",
    "print('precision = ',precision)\n",
    "print('recall = ', recall)\n",
    "print('f1 score = ',f1_score)\n",
    "print('RF accuracy = ',acc)\n",
    "\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
